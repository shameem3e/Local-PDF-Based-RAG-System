# ğŸ“š Local-PDF-Based RAG System

A **Retrieval-Augmented Generation (RAG)** system that processes **local PDF documents** to answer user queries in a context-aware manner â€” without relying on ChatGPT or paid APIs.  
Simply place your documents in the `docs/` folder, run the ingestion process, and start chatting from the terminal.

---

## ğŸ“Œ Features
- ğŸ“„ **PDF Document Support** â€” Works with `.pdf`, `.txt`, `.md` files.
- ğŸ” **Semantic Search** â€” Uses embeddings + vector database for relevant chunk retrieval.
- ğŸ§  **Local LLM** â€” Runs on pre-trained Hugging Face models (no internet needed after setup).
- ğŸ’¬ **Interactive Chat Mode** â€” Terminal-based chatbot for follow-up questions.
- âš¡ **Fast Retrieval** â€” Powered by FAISS for quick vector search.
- ğŸ›  **Customizable** â€” Change chunk size, overlap, or switch to larger models easily.

---

## ğŸ“‚ Folder Structure
my_rag_project/

â”œâ”€ docs/ # Place your PDF/TXT/MD files here

â”œâ”€ persist/ # Saved embeddings, index, and metadata

â”œâ”€ src/

â”‚ â”œâ”€ utils.py # PDF text extraction & chunking

â”‚ â”œâ”€ ingest.py # Embeds docs & stores in FAISS

â”‚ â”œâ”€ retriever.py # Retrieves top-matching chunks

â”‚ â”œâ”€ generator.py # Generates answer using local LLM

â”‚ â””â”€ app.py # Main entry: query or chat mode

â”œâ”€ requirements.txt # Python dependencies

â””â”€ README.md # Project documentation


---

## âš™ï¸ Setup Guide

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/shameem3e/Local-PDF-Based-RAG-System.git
cd Local-PDF-Based-RAG-System

```
### **2ï¸âƒ£ Create Virtual Environment**
```bash
python -m venv .venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

```
### **3ï¸âƒ£ Install Requirements**
```bash
pip install -r requirements.txt

```
### **4ï¸âƒ£ Add Documents**
Place your `.pdf`, `.txt`, or `.md` files into the `docs/` folder.

### **5ï¸âƒ£ Ingest Documents**
```bash
python src/ingest.py

```
### **6ï¸âƒ£ Query or Chat**
* Single Question
```bash
python src/app.py query --question "YOUR QUESTION?"

```
* Chat Mode
```bash
python src/app.py chat

```
Type your question from the PDF and chat with your RAG system.

## ğŸ“œ Code Overview
| File             | Description                                                                |
| ---------------- | -------------------------------------------------------------------------- |
| **utils.py**     | Extracts and chunks text from PDF/TXT/MD files.                            |
| **ingest.py**    | Converts chunks into embeddings and saves them into FAISS index.           |
| **retriever.py** | Finds most relevant text chunks based on the query.                        |
| **generator.py** | Uses a Hugging Face model to generate final answer from retrieved context. |
| **app.py**       | CLI tool for ingestion, querying, and interactive chatting.                |

## â“ FAQ
Q: Do I need the internet to run it?

A: Only for the first model download. After that, it works offline.

Q: Can I use other file types?

A: Yes, `.txt` and `.md` work. For others, add parsing logic in `utils.py`.

Q: Itâ€™s slow, what can I do?

A: Use a smaller model (`flan-t5-small`) or run on GPU.

Q: FAISS not installing?

A: Use `pip install faiss-cpu` or switch to `scikit-learn` fallback.

## ğŸ›  Tech Stack
* Python 3.9+
* FAISS â€” Vector database for fast similarity search
* Sentence-Transformers â€” Embedding generation
* Hugging Face Transformers â€” Local language models
* PyPDF2 / pdfplumber â€” PDF text extraction

## ğŸš€ Future Improvements
* Add Google Search as optional data source
* Support multimodal RAG (PDF + Images)
* Add web UI instead of terminal
* Improve answer summarization

## ğŸ‘¨â€ğŸ’» Author
[MD. Shameem Ahammed](https://sites.google.com/view/shameem3e)

Graduate Student | AI & ML Enthusiast

---
